---
title: Create a Virtual GPU cluster
sidebarTitle: Create a Virtual GPU cluster
---

Virtual GPU clusters provide GPU computing resources through virtualization, offering flexibility in configuration and resource allocation. Each cluster consists of one or more virtual machines with dedicated GPU access.

For an overview of GPU cluster types and their differences, see [About GPU Cloud](/edge-ai/ai-infrastructure/about-gpu-cloud).

## Cluster architecture

Each Virtual GPU cluster consists of one or more virtual machine nodes. All nodes are created from an identical template (image, network settings, disk configuration). After creation, individual nodes can have their disk and network configurations modified independently.

For flavors with InfiniBand support, high-speed inter-node networking is configured automatically. This enables efficient distributed training across multiple nodes without manual network configuration.

Each node has:

- A **network boot disk** (required). At least one network disk is required as the boot volume for the operating system.
- A **local data disk** added by default. This non-replicated disk is dedicated to temporary storage.
- Optional **network data disks** that can be attached during creation or added later. Network disks persist independently of node state.
- Optional **file share** integration for shared storage across instances.

<Warning>
The local data disk is a non-replicated volume that comes with every Virtual GPU instance. This disk:

- Cannot be modified, detached, or used as a boot volume
- Is strictly bound to the specific virtual machine and its configuration
- Is wiped when the node is reconfigured, powered off (shelved), or deleted

Use this disk only for temporary data. Store all important data on network disks or NFS storage to prevent data loss.
</Warning>


## Create a Virtual GPU cluster

To create a Virtual GPU cluster, complete the following steps in the Gcore Customer Portal.

1. In the [Gcore Customer Portal](https://portal.gcore.com), navigate to **GPU Cloud**.
2. In the sidebar, expand **GPU Clusters** and select **Virtual GPU Clusters**.
3. Click **Create Cluster**.


### Step 1. Select region

In the **Region** section, select the data center location for the cluster.


Regions are grouped by geography (Asia-Pacific, EMEA). Each region card shows its availability status.

<Info>
GPU model availability and pricing vary by region. Check other regions if a required GPU model is not available. For help with availability, contact the [sales team.](https://gcore.com/contact-sales)
</Info>

### Step 2. Select cluster type

In the **GPU Cluster type** section, select **Virtual GPU** to create a Virtual GPU cluster.

The available cluster types are:

- **Virtual GPU**: Clusters of virtual instances with dedicated GPU resources
- **Bare Metal GPU**: Dedicated clusters of physical GPU servers
- **Spot Bare Metal GPU**: Discounted bare metal GPU servers with non-guaranteed availability

Selecting **Virtual GPU** displays flavors and options specific to Virtual GPU clusters.

### Step 3. Configure cluster capacity

Cluster capacity determines the hardware specifications for each node in the cluster.

1. Select the **GPU Model**. Available models depend on the region.

2. Enable or disable **Show out of stock** to filter available flavors.

3. Select a flavor. Each flavor card displays GPU configuration, vCPU count, RAM capacity, and pricing.

### Step 4. Set the number of instances

In the **Number of Instances** section, specify how many virtual machines to provision in the cluster.

<Frame>![Number of Instances selector showing increment and decrement buttons](/images/docs/edge-ai/ai-infrastructure/create-a-virtual-gpu-cluster/virtual-gpu-instances-count.png)</Frame>

Each instance is a separate virtual machine with the selected flavor configuration. When created, all instances have identical configurations. The minimum is 1 instance, maximum depends on regional availability.

### Step 5. Select image

The image defines the operating system and pre-installed software for cluster nodes.

<Frame>![Image selector with Public and Custom tabs and image dropdown](/images/docs/edge-ai/ai-infrastructure/create-a-virtual-gpu-cluster/virtual-gpu-image-selector.png)</Frame>

1. In the **Image** section, choose the operating system:
   - **Public**: Pre-configured images with NVIDIA drivers and CUDA toolkit
   - **Custom**: Custom images uploaded to the account

The default Ubuntu images include pre-installed NVIDIA drivers and CUDA toolkit. Images with the `eni` suffix in the name are configured for InfiniBand interconnect.

2. Note the default login credentials displayed below the image selector.


### Step 6. Configure volumes

Each Virtual GPU cluster instance has the following storage:

- **System volume** (required): The boot volume for the operating system. This network disk persists across power cycles.
- **Local volume**: Added automatically to every instance. This non-replicated disk is dedicated to temporary storage only. It cannot be edited, removed, or used as a boot volume. Data on this disk is deleted when the instance is shelved, reconfigured, or deleted.
- **Additional network volumes** (optional): Additional persistent storage that survives power cycles.

<Frame>![Volumes configuration section with volume type, size, and configuration options](/images/docs/edge-ai/ai-infrastructure/create-a-virtual-gpu-cluster/virtual-gpu-volumes-config.png)</Frame>

To configure volumes:

1. Configure the **System** volume:
   - **Name**: Enter a name for the volume
   - **Size**: Minimum size depends on the selected image (default: 120 GiB)
   - **Type**: Select from available storage types in the region (for example, High IOPS SSD)

2. To add additional network volumes, click **Add Volume**.

3. For each additional volume, configure the name, size, and type.

All configured network volumes are attached to every instance in the cluster at creation.

<Warning>
Store important data on network volumes or file shares, not on the local volume. The local volume is wiped when the instance is shelved, reconfigured, or deleted, resulting in permanent data loss.
</Warning>

<Info>
After cluster creation, network volumes can be managed individually per instance. Additional volumes can be added to specific instances, and existing volumes can be detached from individual instances.
</Info>

### Step 6a. Configure file share integration (optional)

The **File share integration** section allows you to connect a shared file system that is accessible across all instances in the cluster.

Enabling file share integration is recommended for workflows that require shared data access, such as distributed training or shared datasets.

To enable file share integration:

1. Select the **Enable File Share integration** checkbox.
2. Select an existing file share or create a new one.

<Info>
When file share integration is enabled, the maximum number of network interfaces per instance is reduced from 3 to 2, because one interface is reserved for NFS connectivity.
</Info>

For detailed information about file shares, see [Configure file shares](/cloud/file-shares/configure-file-shares).

### Step 7. Configure network settings

Network settings define how the cluster communicates with external services and other resources. At least one interface is required.

<Info>
Virtual GPU clusters support a maximum of 3 network interfaces (Public and Private combined) per instance. If file share integration is enabled, the limit is reduced to 2 interfaces, because one interface is reserved for NFS connectivity.

InfiniBand interfaces are not counted toward this limit; they are configured automatically based on the selected flavor.
</Info>

1. In the **Network settings** section, configure the network interface:

| Type | Access | Use case |
|------|--------|----------|
| **Public** | Direct internet access with dynamic public IP | Development, testing, quick access to cluster |
| **Private** | Internal network only, no external access | Production workloads, security-sensitive environments |
| **Dedicated public** | Reserved static public IP | Production APIs, services requiring stable endpoints |

To add additional interfaces, click **Add Interface**. When the maximum number of interfaces is reached, the button is disabled.

For detailed networking configuration, see [Create and manage a network.](/cloud/networking/create-and-manage-a-network)

### Step 7a. Configure firewall settings

The **Firewall settings** section allows you to apply firewall rules to control inbound and outbound traffic for the cluster instances.

1. In the **Firewall settings** section, select one or more firewalls from the dropdown.
2. To add more firewalls, click **Add Firewall**.

A default firewall is pre-selected. You can remove it and select different firewalls as needed.

For detailed information about configuring firewall rules, see [Add and configure a firewall](/cloud/networking/add-and-configure-a-firewall).

### Step 8. Configure SSH key

In the **SSH key** section, select an existing key from the dropdown or create a new one. Keys can be uploaded or generated directly in the portal.

<Frame>![SSH key selector and Firewall settings section](/images/docs/edge-ai/ai-infrastructure/create-a-virtual-gpu-cluster/virtual-gpu-ssh-firewall.png)</Frame>

If generating a new key pair, save the private key immediately as it cannot be retrieved later.

### Step 9. Set additional options

The **Additional options** section provides optional settings: user data scripts for automated configuration and metadata tags for resource organization.


### Step 10. Name and create the cluster

The final step assigns a name to the cluster and initiates provisioning.


1. In the **GPU Cluster Name** section, enter a name or use the auto-generated one.

2. Review the **Estimated cost** panel on the right.

3. Click **Create Cluster**.

Once all instances reach **Power on** status, the cluster is ready for use.

<Warning>
Cluster-level settings (image, default networks) cannot be changed after creation. New nodes added via scaling inherit the original configuration. To change these settings, create a new cluster.
</Warning>

## Connect to the cluster

After the cluster is created, use SSH to access the nodes. The default username is `ubuntu`.

```bash
ssh ubuntu@<node-ip-address>
```

Replace `<node-ip-address>` with the public or floating IP shown in the cluster details.

For nodes with only private interfaces, connect through a bastion host or VPN, or use the [Gcore Customer Portal console.](/cloud/virtual-instances/connect/connect-to-your-instance-via-control-panel)

## Verify cluster status

After connecting, verify that GPUs are available and drivers are loaded:

```bash
nvidia-smi
```

The output displays all available GPUs, driver version, and CUDA version. If no GPUs appear, check that the image includes the correct NVIDIA drivers for the GPU model.

## Manage cluster power state

Virtual GPU clusters support power management operations at the cluster level. Unlike Bare Metal clusters, powering off a Virtual GPU cluster releases compute resources (shelving), which stops billing but also removes data from local disks.

<Frame>![Power tab showing Power on, Power off, Soft reboot, and Hard reboot options](/images/docs/edge-ai/ai-infrastructure/create-a-virtual-gpu-cluster/virtual-gpu-power-tab.png)</Frame>

### Power off (shelve) a cluster

Powering off a Virtual GPU cluster shelves all nodes. Shelving releases CPU, RAM, GPU, and local disk resources. No charges apply while nodes are shelved.

1. In the cluster list or cluster details page, locate the cluster.
2. Click the **Power Off** action.
3. Confirm the operation.

<Warning>
When a node is shelved, the local data disk is deleted and all data on it is lost. Only network disks persist. Save important data to network disks or external storage before powering off.
</Warning>

### Power on a cluster

Powering on a shelved cluster attempts to allocate resources for all nodes.

1. In the cluster list or cluster details page, locate the powered-off cluster.
2. Click the **Power On** action.

<Warning>
Restart is not guaranteed after shelving. If the requested resources (GPU model, flavor) are not available in the region at the time of power on, the operation fails. For workloads requiring guaranteed availability, use Bare Metal clusters.
</Warning>


## Change cluster flavor

Virtual GPU clusters support changing the flavor after creation, allowing adjustment of GPU, CPU, and RAM allocation without recreating the cluster. This is not available for Bare Metal clusters.


1. Power off (shelve) the cluster. Flavor changes require the cluster to be in shelved state.
2. In the cluster details page, click **Change Flavor**.
3. Select the new flavor from available options.
4. Confirm the change.
5. Power on the cluster.

<Info>
Flavor availability depends on the region and current capacity. If the desired flavor is not available, the change cannot be applied.
</Info>


## Manage node disks

After cluster creation, disks can be managed individually for each node. This allows adding storage to specific nodes or removing unused disks.

<Frame>![Cluster details page showing Overview, Power, Volumes, Networking, Tags, and Delete tabs](/images/docs/edge-ai/ai-infrastructure/create-a-virtual-gpu-cluster/virtual-gpu-cluster-details.png)</Frame>

### Add a disk to a node

1. Navigate to the cluster details page.
2. Open the **Volumes** tab.
3. Select the node to attach a disk to.
4. Click **Add Volume** and configure the disk type and size.
5. Click **Create** to attach the new disk.

### Remove a disk from a node

1. Navigate to the cluster details page.
2. Open the **Volumes** tab.
3. Locate the disk to remove and click the detach action.
4. Confirm the operation.

<Warning>
Detaching a disk does not delete it. The disk remains in the account and continues to incur storage charges until explicitly deleted.
</Warning>

## Manage network interfaces

After cluster creation, network interfaces can be managed individually for each node through the **Networking** tab on the cluster details page.

### Interface limits

Virtual GPU clusters support a maximum of 3 network interfaces (Public and Private combined) per node. If Vast file share integration is enabled, the limit is reduced to 2 interfaces.

InfiniBand interfaces are not counted toward this limit; they are managed automatically and cannot be modified.

### Add an interface to a node

1. Navigate to the cluster details page.
2. Open the **Networking** tab.
3. Click on a node to expand its details.
4. Click **Add Interface**.
5. Configure the interface type (Public or Private) and IP allocation settings.

When a node has the maximum number of interfaces, the **Add Interface** button is disabled.

### Remove an interface from a node

1. Navigate to the cluster details page.
2. Open the **Networking** tab.
3. Click on a node to expand its details.
4. Locate the interface to remove and click the delete action.
5. Confirm the operation.

<Info>
InfiniBand interfaces cannot be removed or modified. They are managed automatically to ensure inter-node communication.
</Info>

## Delete a cluster

When deleting a Virtual GPU cluster, disks attached to nodes can optionally be preserved.

<Frame>![Delete Cluster dialog with options to select which volumes to delete](/images/docs/edge-ai/ai-infrastructure/create-a-virtual-gpu-cluster/virtual-gpu-delete-dialog.png)</Frame>

1. In the cluster list or cluster details page, click **Delete**.
2. In the confirmation dialog:
   - To delete all disks along with the cluster, leave the **Delete disks** checkbox selected.
   - To preserve disks for later use, clear the **Delete disks** checkbox.
3. Confirm the deletion.


<Info>
Preserved disks remain in the account and continue to incur storage charges. They can be attached to other instances or clusters.
</Info>

## Automating cluster management

The [Gcore Customer Portal](https://portal.gcore.com) is suitable for creating and managing individual clusters. For automated workflows—such as CI/CD pipelines, infrastructure-as-code, or batch provisioning—use the GPU Virtual API.

The API allows:

- Creating and deleting clusters programmatically
- Starting, stopping, and rebooting clusters
- Changing cluster flavor
- Managing volumes attached to cluster servers
- Querying available GPU flavors and regions

For authentication, request formats, and code examples, see the [GPU Virtual API reference.](/api-reference/cloud/gpu-virtual/create-virtual-gpu-cluster)

