---
title: "Configuring file shares"
sidebarTitle: "Configure"
---

Create NFS-based file shares and mount them to your Linux virtual machines, bare metal servers, or GPU clusters.

Two types of file shares are available:

- **Standard** are general-purpose file shares using a Ceph-based backend.
- **VAST** are high-performance type file shares, available in selected GPU-enabled regions.

<Info>
  File share types are **mutually exclusive**, meaning each region has either Standard **or** VAST file shares, but not both.
</Info>

<Tip>
  The best practice is to create VAST shares **when** creating [GPU clusters](/edge-ai/ai-infrastructure/create-an-ai-cluster) or **before** provisioning the corresponding [compute resources](/cloud/virtual-instances/types-of-virtual-machines) (such as VMs).
</Tip>

The creation flow for both types starts the same. Use the steps below and follow the instructions for your selected file share type.

<Info>
  Please ensure that there are enough quotas to create the file share. To increase quotas, send us a request according to our guide. The quotas for File Shares are located on the Storage tab and include File Share count and File Share size (GiB).
</Info>

<Frame>
  ![Storage tab ](/images/docs/cloud/file-shares/1.png)
</Frame>

## Prepare your network for Bare Metal

If you plan to mount the file share on a bare-metal server, the network must support bare-metal, and these servers require a dedicated VLAN.

If needed, create a new network and enable the **Bare Metal Network** toggle during configuration.

<Frame>
  ![Bare Metal Network toggle](/images/docs/cloud/file-shares/2-1.png)
</Frame>

<Info>
  You must manually change the OS settings' existing Bare Metal network interface.
</Info>

## Configure file shares for Linux VMs and Bare Metal

This section describes creating and connecting a standard NFS-based file share using a private network. It can be used with Linux virtual machines or bare-metal servers.

### Step 1. Create a file share

1. In the **Cloud** menu, click **Storage**, select **File Shares**, and click **Create File Share** on the right.

<Frame>
  ![File Shares](/images/docs/cloud/file-shares/3-1.png)
</Frame>

2. In the **Basic settings** panel, enter _File Share name_, specify _Size_, and select **Standard** as the _File Share type_.

![File Shares Standard 4 Pn](/images/file-shares-standard-4.png)

3. In the **Network settings** panel, select the private _Network_ and _Subnetwork_ to use for the file share.
4. In the **Access** panel, click **Add rule** and specify the IP addresses of the resources that should have access to the file share, and their access modes.
5. Set **Additional options**, if required.

### Step 2. Set up NFS client support

Connect to your server via the [Gcore Customer Portal](/cloud/virtual-instances/connect/connect-to-your-instance-via-control-panel) or SSH and set up NFS client support.

For Ubuntu and Debian:

`sudo apt -y install nfs-common`

For CentOS:

`sudo yum install nfs-utils`

### Step 3. Mount the file share

1. Use an existing directory for mounting the share, or create a new one, for example:

   `sudo mkdir -p /mount/path`
2. Copy the mount command from the file share **Overview** tab.

<Frame>
  ![Mount the file share](/images/docs/cloud/file-shares/4-1.jpg)
</Frame>

Mount the file share:

`mount 0.0.0.0:/shares/share-d54589b8-132f-4de4-ae99-af5c6cdfdb9c /mount/path`

Replace `/mount/path` with the absolute local directory path where you want the file share to be mounted.

## VAST file shares

<Info>
  VAST-based file shares are only available in regions with GPU support, and use high-performance backend designed for intensive data workloads.
</Info>

<Tip>
  The best practice is to create VAST shares **when** creating [GPU clusters](/edge-ai/ai-infrastructure/create-an-ai-cluster) or **before** provisioning the corresponding [compute resources](/cloud/virtual-instances/types-of-virtual-machines) (such as VMs).
</Tip>

### Add VAST file share during GPU cluster creation

When creating a GPU cluster, you can add a VAST file share directly in the cluster creation wizard. This is the recommended approach as network interfaces and mounting are configured automatically.

#### Step 1. Enable file share integration

Follow the [GPU cluster creation guide](/edge-ai/ai-infrastructure/create-an-ai-cluster). When you reach the **File share integration** section, enable the toggle.

<Frame>![Enable File Share integration toggle](/images/docs/cloud/file-shares/vast-cluster-enable-toggle.png)</Frame>

#### Step 2. Select file share and mount path

Select an existing VAST file share or create a new one, and specify the **Mount path** where the file share will be accessible on your cluster nodes.

<Frame>![Select file share and configure mount path](/images/docs/cloud/file-shares/vast-cluster-select-fileshare.png)</Frame>

After selecting a VAST file share, the required network interfaces are automatically added to each node in your cluster.

#### Step 3. Configure user data (optional)

By default, **User data** is disabled and the file share will be mounted automatically when the cluster starts.

<Frame>![User data disabled - file share mounts automatically](/images/docs/cloud/file-shares/vast-cluster-userdata-disabled.png)</Frame>

If **User data** is enabled, mounting commands are included in the user data script.

<Frame>![User data enabled with auto-generated mount commands](/images/docs/cloud/file-shares/vast-cluster-userdata-enabled.png)</Frame>

<Warning>
  If user data is enabled, do not modify or delete the auto-generated mounting commands. Changing these commands may break automatic mounting.
</Warning>

### Add VAST file share manually

If you need to add a VAST file share to an existing resource or prefer manual configuration, follow the steps below.

### Step 1. Create a VAST share

1. In the **Cloud** menu, click **Storage**, select **File Shares**, and click **Create File Share** on the right.
2. In the **Basic settings** panel, enter _File Share name_, specify _Size_, and select **VAST** as the _File Share type_.
3. Set **Additional options**, if required.

![File Share Vast Pn](/images/file-share-vast.png)

When VAST share type is selected, controls in **Network settings** and **Access** panels are disabled, and the network is assigned automatically.

The access rules cannot be configured manually â€” the VAST share is always available in read/write mode within its assigned network only. This network is not visible in the general **Network** tab. It is only available when attaching interfaces to a resource (VM, Bare Metal, or GPU cluster).

VAST file shares always use read/write access; access rules are not supported.

### Step 2. Add VAST network interface to a compute resource

Once the VAST file share is created, it is best to add the VAST network interface while provisioning the corresponding GPU cluster or compute resource.

#### Adding VAST interface while creating a compute resource

While provisioning a compute resource:

1. Scroll down to the **Network settings** panel.
2. Click **Add interface**.
3. Click the interface to configure, and select the **VAST network**.
4. Continue with provisioning the compute resource.

The VAST network only becomes available after the file share has been created. It is a third, dedicated network, separate and distinct from public and private networks.

![File Share Details Pn](/images/file-share-details.png)

#### Adding VAST interface to an existing compute resource

While the VAST interface can be attached to an already-provisioned GPU cluster or compute resource, this approach is generally **not recommended** because of additional complexity.

<Tip>
  The best practice is to create VAST shares **when** creating [GPU clusters](/edge-ai/ai-infrastructure/create-an-ai-cluster) or **before** provisioning the corresponding [compute resources](/cloud/virtual-instances/types-of-virtual-machines) (such as VMs).
</Tip>

**Attach VAST network interface**

1. Go to server **Resource** settings ([VM](/cloud/virtual-instances/create-an-instance), [Bare Metal](/cloud/bare-metal-servers/create-a-bare-metal-server), or [GPU cluster](/edge-ai/ai-infrastructure/create-an-ai-cluster)).
2. Select the **Networking** tab and click **Add interface**.
4. Click the **Network** drop-down and select the **VAST network**, then click **Add**.
6. Once the interface is added, **note the following details** for use in subsequent steps:
   - **IP** address (such as `198.51.100.25`)
   - **MAC** address (such as `fa:16:3e:12:34:56`)
   - **CIDR** range (such as `198.51.102.0/22`)

**Configure attached VAST network interface**

<Info>
  This configuration is required only when adding an interface to an existing, already provisionned resource; for interfaces added during resource creation and provisioning, this configuration is performed automatically.
</Info>

Connect to your server via the [Gcore Customer Portal](/cloud/virtual-instances/connect/connect-to-your-instance-via-control-panel) or SSH and configure the attached VAST network interface.

1. Use `ip addr` to list all available interfaces.
2. Identify the VAST interface using the MAC address from the previous step.
3. Note the **interface name** for use in the following steps.
4. Configure the interface using the commands for your instance type:

<Tabs>
  <Tab title="VM / GPU VM">
    ```bash
    # Replace $interface with interface name (e.g., enp4s0)
    # Replace $ipcidr with IP/CIDR from Step 2 (e.g., 172.18.220.100/22)
    # Replace $subnetgw with first IP in subnet (e.g., 172.18.220.1)

    sudo ip link set $interface mtu 9000
    sudo ip link set $interface up
    sudo ip addr add $ipcidr dev $interface
    sudo ip route add 172.19.252.0/22 via $subnetgw dev $interface
    ```

    **Example:**

    ```bash
    sudo ip link set enp4s0 mtu 9000
    sudo ip link set enp4s0 up
    sudo ip addr add 172.18.220.100/22 dev enp4s0
    sudo ip route add 172.19.252.0/22 via 172.18.220.1 dev enp4s0
    ```
  </Tab>
  <Tab title="Bare Metal with Bluefield DPU">
    ```bash
    # Replace $interface with interface name (e.g., ens10f0v0)
    # Replace $ipcidr with IP/CIDR from Step 2 (e.g., 172.18.220.100/22)
    # Replace $subnetgw with first IP in subnet (e.g., 172.18.220.1)

    sudo ip link set $interface mtu 9000
    sudo ip link set $interface up
    sudo ip addr add $ipcidr dev $interface
    sudo ip route add 172.19.252.0/22 via $subnetgw dev $interface
    ```

    **Example:**

    ```bash
    sudo ip link set ens10f0v0 mtu 9000
    sudo ip link set ens10f0v0 up
    sudo ip addr add 172.18.220.100/22 dev ens10f0v0
    sudo ip route add 172.19.252.0/22 via 172.18.220.1 dev ens10f0v0
    ```
  </Tab>
  <Tab title="Bare Metal with Arista (Trunks)">
    Create a VLAN interface on the bond:

    ```bash
    # Replace $vlan_id with VLAN ID from UI/API (e.g., 2998)
    # Replace $ipcidr with IP/CIDR from Step 2 (e.g., 172.19.137.8/22)
    # Replace $subnetgw with first IP in subnet (e.g., 172.19.136.1)

    sudo ip link add link bond0 name bond0.$vlan_id type vlan id $vlan_id
    sudo ip link set bond0.$vlan_id up
    sudo ip addr add $ipcidr dev bond0.$vlan_id
    sudo ip route add 172.19.252.0/22 via $subnetgw dev bond0.$vlan_id
    ```

    **Example:**

    ```bash
    sudo ip link add link bond0 name bond0.2998 type vlan id 2998
    sudo ip link set bond0.2998 up
    sudo ip addr add 172.19.137.8/22 dev bond0.2998
    sudo ip route add 172.19.252.0/22 via 172.19.136.1 dev bond0.2998
    ```
  </Tab>
</Tabs>

<Info>
  The route `ip route add 172.19.252.0/22` is the same for all instance types.
</Info>

### Step 3. Set up VAST NFS client support

<Info>
  This step is **always required** for VAST file shares, regardless of whether they are added during or after resource provisioning.
</Info>

Connect to your server via the [Gcore Customer Portal](/cloud/virtual-instances/connect/connect-to-your-instance-via-control-panel) or SSH and set up VAST NFS client support.

1. Install NFS client tools:

`sudo apt-get update && sudo apt-get install nfs-common -y`

2. Install build tools and headers for kernel modules:

`sudo apt install dkms debhelper dh-dkms build-essential linux-headers-$(uname -r) -y`

3. Install VAST NFS package:

`curl -sSf https://vast-nfs.s3.amazonaws.com/download.sh | bash -s --`

### Step 4. Mount the file share

You can use an existing directory for mounting the share, or create a new one, for example:

`sudo mkdir -p /mount/path`

Mount the VAST file share:

- Replace `ndp1-2-vast.cloud.gc.onl:/manila/manila-01234567-89ab-cdef-0123-456789abcdef` with the connection point from the VAST file share overview tab.
- Replace `/mount/path` with the absolute local directory path where you want the file share to be mounted.

`sudo mount -o vers=3,nconnect=56,remoteports=dns,spread_reads,spread_writes,noextend ndp1-2-vast.cloud.gc.onl:/manila/manila-01234567-89ab-cdef-0123-456789abcdef /mount/path`

Now you can access the contents of the VAST file share in the specified directory.

<Info>
  Always use NFS version 3 (vers=3) when mounting VAST file shares. If your system does not support the `nconnect` option, install the[ VAST Enhanced NFS Client](https://vastnfs.vastdata.com/docs/4.0/download.html).
</Info>

![File Share Mount Pn](/images/file-share-mount.png)

## Resizing file shares

<Info>
  - **VAST** shares can be resized directly, without being unmounted.
  - **Standard** shares must first be unmounted before they can be resized, for example:

    `umount -lf /mount/path`

    Once resized, file shares can be remounted as described above for each share type.
</Info>

### Resize file share

This process is the same for both Standard and VAST-based file shares:

1. In the **Cloud** menu, go to the **Storage** tab, select **File Shares**, and click the file share to be resized.
2. In the **Overview** tab, click **Resize** and enter the new share size:

<Frame>
  ![Overview tab](/images/docs/cloud/file-shares/5-1.png)
</Frame>