---
title: "Configuring file shares"
sidebarTitle: "Configure"
---

Create NFS-based file shares and mount them to Linux virtual machines, bare metal servers, or GPU clusters.

Two types of file shares are available:

- **Standard** are general-purpose file shares using a Ceph-based backend.
- **VAST** are high-performance type file shares, available in selected GPU-enabled regions.

<Info>
  File share types are **mutually exclusive**, meaning each region has either Standard **or** VAST file shares, but not both.
</Info>

<Tip>
  The best practice is to create VAST shares **when** creating [GPU clusters](/edge-ai/ai-infrastructure/create-a-bare-metal-gpu-cluster) or **before** provisioning the corresponding [compute resources](/cloud/virtual-instances/types-of-virtual-machines) (such as VMs).
</Tip>

The creation flow for both types starts the same. Use the steps below and follow the instructions for the selected file share type.

<Info>
  Please ensure that there are enough quotas to create the file share. To increase quotas, send us a request according to our guide. The quotas for File Shares are located on the Storage tab and include File Share count and File Share size (GiB).
</Info>

<Frame>
  ![Storage tab ](/images/docs/cloud/file-shares/1.png)
</Frame>

## Prepare the network for Bare Metal

To mount the file share on a bare-metal server, the network must support bare-metal, and these servers require a dedicated VLAN.

If needed, create a new network and enable the **Bare Metal Network** toggle during configuration.

<Frame>
  ![Bare Metal Network toggle](/images/docs/cloud/file-shares/2-1.png)
</Frame>

<Info>
  Manually change the OS settings' existing Bare Metal network interface.
</Info>

## Configure file shares for Linux VMs and Bare Metal

This section describes creating and connecting a standard NFS-based file share using a private network. It can be used with Linux virtual machines or bare-metal servers.

### Step 1. Create a file share

1. In the **Cloud** menu, click **Storage**, select **File Shares**, and click **Create File Share** on the right.

<Frame>
  ![File Shares](/images/docs/cloud/file-shares/3-1.png)
</Frame>

2. In the **Basic settings** panel, enter _File Share name_, specify _Size_, and select **Standard** as the _File Share type_.

![File Shares Standard 4 Pn](/images/file-shares-standard-4.png)

3. In the **Network settings** panel, select the private _Network_ and _Subnetwork_ to use for the file share.
4. In the **Access** panel, click **Add rule** and specify the IP addresses of the resources that should have access to the file share, and their access modes.
5. Set **Additional options**, if required.

### Step 2. Set up NFS client support

Connect to the server via the [Gcore Customer Portal](/cloud/virtual-instances/connect/connect-to-your-instance-via-control-panel) or SSH and set up NFS client support.

For Ubuntu and Debian:

```bash
sudo apt -y install nfs-common
```

For CentOS:

```bash
sudo yum install nfs-utils
```

### Step 3. Mount the file share

1. Use an existing directory for mounting the share, or create a new one, for example:

   ```bash
   sudo mkdir -p /mount/path
   ```

2. Copy the mount command from the file share **Overview** tab.

<Frame>
  ![Mount the file share](/images/docs/cloud/file-shares/4-1.jpg)
</Frame>

Mount the file share:

```bash
mount 0.0.0.0:/shares/share-d54589b8-132f-4de4-ae99-af5c6cdfdb9c /mount/path
```

Replace `/mount/path` with the absolute local directory path where the file share should be mounted.

## VAST file shares

<Info>
  VAST-based file shares are only available in regions with GPU support and use a high-performance backend designed for intensive data workloads.
</Info>

<Tip>
  The best practice is to create VAST shares **when** creating [GPU clusters](/edge-ai/ai-infrastructure/create-a-bare-metal-gpu-cluster) or **before** provisioning the corresponding [compute resources](/cloud/virtual-instances/types-of-virtual-machines) (such as VMs).
</Tip>

### Step 1. Create a VAST share

1. In the **Cloud** menu, click **Storage**, select **File Shares**, and click **Create File Share** on the right.
2. In the **Basic settings** panel, enter _File Share name_, specify _Size_, and select **VAST** as the _File Share type_.
3. Set **Additional options**, if required.

![File Share Vast Pn](/images/file-share-vast.png)

When VAST share type is selected, controls in **Network settings** and **Access** panels are disabled, and the network is assigned automatically.

The access rules cannot be configured manually â€” the VAST share is always available in read/write mode within its assigned network only. This network is not visible in the general **Network** tab. It is only available when attaching interfaces to a resource (VM, Bare Metal, or GPU cluster).

VAST file shares always use read/write access; access rules are not supported.

### Step 2. Add VAST network interface to a compute resource

Once the VAST file share is created, it is best to add the VAST network interface while provisioning the corresponding GPU cluster or compute resource.

#### Adding VAST interface while creating a compute resource

While provisioning a compute resource:

1. Scroll down to the **Network settings** panel.
2. Click **Add interface**.
3. Click the interface to configure, and select the **VAST network**.
4. Continue with provisioning the compute resource.

The VAST network only becomes available after the file share has been created. It is a third, dedicated network, separate and distinct from public and private networks.

![File Share Details Pn](/images/file-share-details.png)

#### Adding VAST interface to an existing compute resource

While the VAST interface can be attached to an already-provisioned GPU cluster or compute resource, this requires additional manual network configuration and is not the standard workflow.

<Tip>
  The best practice is to create VAST shares **when** creating [GPU clusters](/edge-ai/ai-infrastructure/create-a-bare-metal-gpu-cluster) or **before** provisioning the corresponding [compute resources](/cloud/virtual-instances/types-of-virtual-machines) (such as VMs).
</Tip>

**Attach VAST network interface**

1. Go to server **Resource** settings ([VM](/cloud/virtual-instances/create-an-instance), [Bare Metal](/cloud/bare-metal-servers/create-a-bare-metal-server), or [GPU cluster](/edge-ai/ai-infrastructure/create-a-bare-metal-gpu-cluster)).
2. Select the **Networking** tab and click **Add interface**.
3. Click the **Network** drop-down and select the **VAST network**, then click **Add**.
4. Once the interface is added, **note the following details** for use in subsequent steps:
   - **IP** address (such as `198.51.100.25`)
   - **MAC** address (such as `fa:16:3e:12:34:56`)
   - **CIDR** range (such as `198.51.102.0/22`)

#### Configure attached VAST network interface

<Info>
  This configuration is required only when adding an interface to an existing, already provisioned resource; for interfaces added during resource creation and provisioning, this configuration is performed automatically.
</Info>

After attaching a VAST network interface to a compute resource, configure it to establish connectivity. Gcore now provides DHCP configuration for VAST subnets, making the setup process much simpler.

**Automatic configuration with DHCP (recommended)**

<Info>
DHCP automatically provides IP address, CIDR, gateway, and routing configuration for the VAST network interface, eliminating the need to manually configure network parameters.
</Info>

For Bare Metal servers, use these commands to configure the VAST interface with DHCP:

```bash
# Create VLAN interface (replace 2998 with the VLAN ID)
ip link add link bond0 name bond0.2998 type vlan id 2998
ip link set bond0.2998 up

# Obtain network configuration via DHCP
dhcpcd bond0.2998
```

The DHCP client automatically obtains the IP address, subnet mask, gateway, and routing information needed to connect to the VAST storage.

#### Manual configuration (alternative method)

<Warning>
**Warning**

Manual configuration requires specific network parameters that are not displayed in the interface. Contact Gcore support if you need the exact IP addresses, CIDR blocks, and gateway information for manual setup.
</Warning>
</Warning>

To configure the interface manually, use the following approach:

Connect to the server via the [Gcore Customer Portal](/cloud/virtual-instances/connect/connect-to-your-instance-via-control-panel) or SSH and configure the attached VAST network interface.

1. Use `ip addr` to list all available interfaces.
2. Identify the VAST interface using the MAC address from the previous step.
3. Note the **interface name** (such as `enp8s0` or `enp4s0`) for use in the following steps.
4. Configure the interface as described below for the relevant instance type.

**Configuring VAST interface for VMs and GPU clusters**

Replace `198.51.100.25` with the IP address from Step 2. Replace `enp8s0` with the interface name from above, if different.

Set interface MTU:

```bash
sudo ip link set enp8s0 mtu 9000
```

Activate the interface:

```bash
sudo ip link set enp8s0 up
```

Set interface CIDR:

```bash
sudo ip addr add 198.51.100.25/22 dev enp8s0
```

Add static route to the VAST network. Replace `198.51.100.1` with an address formed by the first three octets from the CIDR range in Step 2, keeping `1` as the last octet for the gateway IP.

```bash
sudo ip route add 172.19.252.0/22 via 198.51.100.1 dev enp8s0
```

**Configuring VAST interface for Bare Metal servers**

Bare Metal servers require different configuration depending on the network hardware.

<AccordionGroup>
  <Accordion title="Bluefield DPU">
    In the commands below, replace `198.51.100.25` with the IP address from Step 2, and replace `ens10f0v0` with the interface name identified above.

    Set interface MTU:

    ```bash
    sudo ip link set ens10f0v0 mtu 9000
    ```

    Activate the interface:

    ```bash
    sudo ip link set ens10f0v0 up
    ```

    Set interface CIDR:

    ```bash
    sudo ip addr add 198.51.100.25/22 dev ens10f0v0
    ```

    Add static route to the VAST network. Replace `198.51.100.1` with an address formed by the first three octets from the CIDR range in Step 2, keeping `1` as the last octet for the gateway IP.

    ```bash
    sudo ip route add 172.19.252.0/22 via 198.51.100.1 dev ens10f0v0
    ```
  </Accordion>
  <Accordion title="Arista (Trunks)">
    For Bare Metal servers with Arista switches (Trunks), create a VLAN interface on the bond. A bond (link aggregation) combines multiple physical network interfaces into a single logical interface, providing higher throughput and redundancy.

    In the commands below, replace `100` with the VLAN ID shown in the **Networking** tab of the server details page, and replace `198.51.100.25` with the IP address from Step 2.

    Create VLAN interface on a bond:

    ```bash
    sudo ip link add link bond0 name bond0.100 type vlan id 100
    ```

    Activate the interface:

    ```bash
    sudo ip link set bond0.100 up
    ```

    Set interface CIDR:

    ```bash
    sudo ip addr add 198.51.100.25/22 dev bond0.100
    ```

    Add static route to the VAST network. Replace `198.51.100.1` with an address formed by the first three octets from the CIDR range in Step 2, keeping `1` as the last octet for the gateway IP.

    ```bash
    sudo ip route add 172.19.252.0/22 via 198.51.100.1 dev bond0.100
    ```
  </Accordion>
</AccordionGroup>


### Step 3. Set up VAST NFS client support

<Info>
  This step is **always required** for VAST file shares, regardless of whether they are added during or after resource provisioning.
</Info>

Connect to the server via the [Gcore Customer Portal](/cloud/virtual-instances/connect/connect-to-your-instance-via-control-panel) or SSH and set up VAST NFS client support.

1. Install NFS client tools:

   ```bash
   sudo apt-get update && sudo apt-get install nfs-common -y
   ```

2. Install build tools and headers for kernel modules:

   ```bash
   sudo apt install dkms debhelper dh-dkms build-essential linux-headers-$(uname -r) -y
   ```

3. Install VAST NFS package:

   ```bash
   curl -sSf https://vast-nfs.s3.amazonaws.com/download.sh | bash -s --
   ```

### Step 4. Mount the file share

Use an existing directory for mounting the share, or create a new one, for example:

```bash
sudo mkdir -p /mount/path
```

Mount the VAST file share:

- Replace `ndp1-2-vast.cloud.gc.onl:/manila/manila-01234567-89ab-cdef-0123-456789abcdef` with the connection point from the VAST file share overview tab.
- Replace `/mount/path` with the absolute local directory path where the file share should be mounted.

```bash
sudo mount -o vers=3,nconnect=56,remoteports=dns,spread_reads,spread_writes,noextend ndp1-2-vast.cloud.gc.onl:/manila/manila-01234567-89ab-cdef-0123-456789abcdef /mount/path
```

The contents of the VAST file share are now accessible in the specified directory.

<Info>
  Always use NFS version 3 (vers=3) when mounting VAST file shares. If the system does not support the `nconnect` option, install the [VAST Enhanced NFS Client](https://vastnfs.vastdata.com/docs/4.0/download.html).
</Info>

![File Share Mount Pn](/images/file-share-mount.png)

## Resizing file shares

<Info>
  - **VAST** shares can be resized directly, without being unmounted.
  - **Standard** shares must first be unmounted before they can be resized, for example:

    ```bash
    umount -lf /mount/path
    ```

    Once resized, file shares can be remounted as described above for each share type.
</Info>

### Resize file share

This process is the same for both Standard and VAST-based file shares:

1. In the **Cloud** menu, go to the **Storage** tab, select **File Shares**, and click the file share to be resized.
2. In the **Overview** tab, click **Resize** and enter the new share size:

<Frame>
  ![Overview tab](/images/docs/cloud/file-shares/5-1.png)
</Frame>