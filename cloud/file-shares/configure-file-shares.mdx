---
title: "Configuring file shares"
sidebarTitle: "Configure"
---

Create NFS-based file shares and mount them to your Linux virtual machines, bare metal servers, or GPU clusters.

Two types of file shares are available:

- **Standard** are general-purpose file shares using a Ceph-based backend.
- **VAST** are high-performance type file shares, available in selected GPU-enabled regions.

<Info>
  File share types are **mutually exclusive**, meaning each region has either Standard **or** VAST file shares, but not both.
</Info>

<Tip>
  The best practice is to create VAST shares **when** creating [GPU clusters](/edge-ai/ai-infrastructure/create-an-ai-cluster) or **before** provisioning the corresponding [compute resources](/cloud/virtual-instances/types-of-virtual-machines) (such as VMs).
</Tip>

The creation flow for both types starts the same. Use the steps below and follow the instructions for your selected file share type.

<Info>
  Please ensure that there are enough quotas to create the file share. To increase quotas, send us a request according to our guide. The quotas for File Shares are located on the Storage tab and include File Share count and File Share size (GiB).
</Info>

<Frame>
  ![Storage tab ](/images/docs/cloud/file-shares/1.png)
</Frame>

## Prepare your network for Bare Metal

If you plan to mount the file share on a bare-metal server, the network must support bare-metal, and these servers require a dedicated VLAN.

If needed, create a new network and enable the **Bare Metal Network** toggle during configuration.

<Frame>
  ![Bare Metal Network toggle](/images/docs/cloud/file-shares/2-1.png)
</Frame>

<Info>
  You must manually change the OS settings' existing Bare Metal network interface.
</Info>

## Configure file shares for Linux VMs and Bare Metal

This section describes creating and connecting a standard NFS-based file share using a private network. It can be used with Linux virtual machines or bare-metal servers.

### Step 1. Create a file share

1. In the **Cloud** menu, click **Storage**, select **File Shares**, and click **Create File Share** on the right.

<Frame>
  ![File Shares](/images/docs/cloud/file-shares/3-1.png)
</Frame>

2. In the **Basic settings** panel, enter _File Share name_, specify _Size_, and select **Standard** as the _File Share type_.

![File Shares Standard 4 Pn](/images/file-shares-standard-4.png)

3. In the **Network settings** panel, select the private _Network_ and _Subnetwork_ to use for the file share.
4. In the **Access** panel, click **Add rule** and specify the IP addresses of the resources that should have access to the file share, and their access modes.
5. Set **Additional options**, if required.

### Step 2. Set up NFS client support

Connect to your server via the [Gcore Customer Portal](/cloud/virtual-instances/connect/connect-to-your-instance-via-control-panel) or SSH and set up NFS client support.

For Ubuntu and Debian:

`sudo apt -y install nfs-common`

For CentOS:

`sudo yum install nfs-utils`

### Step 3. Mount the file share

1. Use an existing directory for mounting the share, or create a new one, for example:

   `sudo mkdir -p /mount/path`
2. Copy the mount command from the file share **Overview** tab.

<Frame>
  ![Mount the file share](/images/docs/cloud/file-shares/4-1.jpg)
</Frame>

Mount the file share:

`mount 0.0.0.0:/shares/share-d54589b8-132f-4de4-ae99-af5c6cdfdb9c /mount/path`

Replace `/mount/path` with the absolute local directory path where you want the file share to be mounted.

## VAST file shares

<Info>
  VAST-based file shares are only available in regions with GPU support and use a high-performance backend designed for intensive data workloads.
</Info>

<Tip>
  The best practice is to create VAST shares **when** creating [GPU clusters](/edge-ai/ai-infrastructure/create-an-ai-cluster) or **before** provisioning the corresponding [compute resources](/cloud/virtual-instances/types-of-virtual-machines) (such as VMs).
</Tip>

There are two ways to configure VAST file shares:

- **During GPU cluster creation** (recommended): Network interfaces and mounting are configured automatically.
- **After resource creation** (not recommended): Requires manual network configuration and mounting for each instance.

### Add a VAST file share during GPU cluster creation (recommended)

This is the preferred method for GPU clusters. Network interfaces are added automatically, and the file share is mounted without manual intervention.

#### Step 1. Start creating a GPU cluster

Follow the [GPU cluster creation guide](/edge-ai/ai-infrastructure/create-an-ai-cluster). When you reach the **File share integration** section, enable the toggle.

<Frame>![Enable File Share integration toggle](/images/docs/cloud/file-shares/vast-cluster-enable-toggle.png)</Frame>

Select an existing VAST file share or create a new one, and specify the **Mount path** where the file share will be accessible on your cluster nodes.

<Frame>![Select file share and configure mount path](/images/docs/cloud/file-shares/vast-cluster-select-fileshare.png)</Frame>

#### Step 2. Review automatic network configuration

After selecting or creating a VAST file share, the required network interfaces are automatically added to each node in your cluster.

#### Step 3. Configure mounting and user data

By default, **User data** is disabled and the file share will be mounted automatically when the cluster starts.

<Frame>![User data disabled - file share mounts automatically](/images/docs/cloud/file-shares/vast-cluster-userdata-disabled.png)</Frame>

If **User data** is enabled, mounting commands are included in the user data script.

<Frame>![User data enabled with auto-generated mount commands](/images/docs/cloud/file-shares/vast-cluster-userdata-enabled.png)</Frame>

<Warning>
  If user data is enabled, do not modify or delete the auto-generated mounting commands. Changing these commands may break automatic mounting.
</Warning>

### Add a VAST file share to an existing resource (manual configuration)

<Warning>
  This approach requires manual configuration for each instance and is significantly more complex. Use the recommended method above whenever possible.
</Warning>

Use this method when adding a VAST file share to an existing GPU cluster, VM, or Bare Metal server.

#### Step 1. Create a VAST share

1. In the **Cloud** menu, click **Storage**, select **File Shares**, and click **Create File Share** on the right.
2. In the **Basic settings** panel, enter _File Share name_, specify _Size_, and select **VAST** as the _File Share type_.
3. Set **Additional options**, if required.

![File Share Vast Pn](/images/file-share-vast.png)

When VAST share type is selected, controls in **Network settings** and **Access** panels are disabled, and the network is assigned automatically.

The access rules cannot be configured manually. The VAST share is always available in read/write mode within its assigned network only. This network is not visible in the general **Network** tab. It is only available when attaching interfaces to a resource (VM, Bare Metal, or GPU cluster).

VAST file shares always use read/write access; access rules are not supported.

#### Step 2. Add VAST network interface to each instance

For each instance (VM, Bare Metal, or GPU cluster node) that needs access to the file share:

1. Go to the resource **Networking** tab.
2. Click **Add interface**.
3. Select the **VAST network** from the dropdown and click **Add**.
4. Note the following details for use in subsequent steps:
   - **IP** address (such as `172.18.220.100`)
   - **MAC** address (such as `fa:16:3e:12:34:56`)
   - **CIDR** range (such as `172.18.220.0/22`)

![File Share Details Pn](/images/file-share-details.png)

#### Step 3. Configure the network interface at OS level

Connect to your instance via the [Gcore Customer Portal](/cloud/virtual-instances/connect/connect-to-your-instance-via-control-panel) or SSH.

<Tabs>
  <Tab title="VM / GPU VM">
    Identify the interface by MAC address using `ip addr`, then configure:

    ```bash
    # Replace $interface with interface name (e.g., enp4s0)
    # Replace $ipcidr with IP/CIDR from Step 2 (e.g., 172.18.220.100/22)
    # Replace $subnetgw with first IP in subnet (e.g., 172.18.220.1)

    ip link set $interface mtu 9000
    ip link set $interface up
    ip addr add $ipcidr dev $interface
    ip route add 172.19.252.0/22 via $subnetgw dev $interface
    ```

    **Example:**

    ```bash
    ip link set enp4s0 mtu 9000
    ip link set enp4s0 up
    ip addr add 172.18.220.100/22 dev enp4s0
    ip route add 172.19.252.0/22 via 172.18.220.1 dev enp4s0
    ```
  </Tab>
  <Tab title="Bare Metal / GPU with Bluefield DPU">
    Identify the interface by MAC address using `ip addr`, then configure:

    ```bash
    # Replace $interface with interface name (e.g., ens10f0v0)
    # Replace $ipcidr with IP/CIDR from Step 2 (e.g., 172.18.220.100/22)
    # Replace $subnetgw with first IP in subnet (e.g., 172.18.220.1)

    ip link set $interface mtu 9000
    ip link set $interface up
    ip addr add $ipcidr dev $interface
    ip route add 172.19.252.0/22 via $subnetgw dev $interface
    ```

    **Example:**

    ```bash
    ip link set ens10f0v0 mtu 9000
    ip link set ens10f0v0 up
    ip addr add 172.18.220.100/22 dev ens10f0v0
    ip route add 172.19.252.0/22 via 172.18.220.1 dev ens10f0v0
    ```
  </Tab>
  <Tab title="Bare Metal / GPU with Arista (Trunks)">
    Create a VLAN interface on the bond:

    ```bash
    # Replace $vlan_id with VLAN ID from UI/API (e.g., 2998)
    # Replace $ipcidr with IP/CIDR from Step 2 (e.g., 172.19.137.8/22)
    # Replace $subnetgw with first IP in subnet (e.g., 172.19.136.1)

    ip link add link bond0 name bond0.$vlan_id type vlan id $vlan_id
    ip link set bond0.$vlan_id up
    ip addr add $ipcidr dev bond0.$vlan_id
    ip route add 172.19.252.0/22 via $subnetgw dev bond0.$vlan_id
    ```

    **Example:**

    ```bash
    ip link add link bond0 name bond0.2998 type vlan id 2998
    ip link set bond0.2998 up
    ip addr add 172.19.137.8/22 dev bond0.2998
    ip route add 172.19.252.0/22 via 172.19.136.1 dev bond0.2998
    ```
  </Tab>
</Tabs>

<Info>
  The route `ip route add 172.19.252.0/22` is the same for all instance types.
</Info>

#### Step 4. Set up VAST NFS client support

Connect to your instance and install the required packages:

1. Install NFS client tools:

   ```bash
   sudo apt-get update && sudo apt-get install nfs-common -y
   ```

2. Install build tools and headers for kernel modules:

   ```bash
   sudo apt install dkms debhelper dh-dkms build-essential linux-headers-$(uname -r) -y
   ```

3. Install VAST NFS package:

   ```bash
   curl -sSf https://vast-nfs.s3.amazonaws.com/download.sh | bash -s --
   ```

#### Step 5. Mount the file share

Create a mount directory and mount the VAST file share:

```bash
sudo mkdir -p /mount/path
```

Mount the file share using the connection point from the VAST file share **Overview** tab:

```bash
sudo mount -o vers=3,nconnect=56,remoteports=dns,spread_reads,spread_writes,noextend \
  ndp1-2-vast.cloud.gc.onl:/manila/manila-01234567-89ab-cdef-0123-456789abcdef /mount/path
```

Replace `/mount/path` with the absolute local directory path where you want the file share to be mounted.

<Info>
  Always use NFS version 3 (vers=3) when mounting VAST file shares. If your system does not support the `nconnect` option, install the [VAST Enhanced NFS Client](https://vastnfs.vastdata.com/docs/4.0/download.html).
</Info>

![File Share Mount Pn](/images/file-share-mount.png)

## Resizing file shares

<Info>
  - **VAST** shares can be resized directly, without being unmounted.
  - **Standard** shares must first be unmounted before they can be resized, for example:

    `umount -lf /mount/path`

    Once resized, file shares can be remounted as described above for each share type.
</Info>

### Resize file share

This process is the same for both Standard and VAST-based file shares:

1. In the **Cloud** menu, go to the **Storage** tab, select **File Shares**, and click the file share to be resized.
2. In the **Overview** tab, click **Resize** and enter the new share size:

<Frame>
  ![Overview tab](/images/docs/cloud/file-shares/5-1.png)
</Frame>